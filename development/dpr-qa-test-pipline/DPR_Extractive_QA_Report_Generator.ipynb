{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPR_Extractive_QA_Report_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0R8IwSCWYVk"
      },
      "source": [
        "!pip install -q -U transformers[sentencepiece] rouge git+https://github.com/deepset-ai/haystack.git grpcio-tools==1.34.1 spacy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EHR4i9FiCAa"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import spacy\n",
        "import nltk\n",
        "import json\n",
        "import pandas as pd\n",
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "from pprint import pprint\n",
        "from typing import List\n",
        "from haystack import Document\n",
        "from haystack.reader import TransformersReader\n",
        "from haystack.pipeline import ExtractiveQAPipeline \n",
        "from haystack.retriever.dense import DensePassageRetriever \n",
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyHgtCYAXEZb",
        "outputId": "90c94379-f827-4492-ae00-9f369a228a06"
      },
      "source": [
        "!spacy download en_core_web_md \n",
        "!spacy link en_core_web_md en"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.1.0/en_core_web_md-3.1.0-py3-none-any.whl (45.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 45.4 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (21.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[31mDeprecationWarning: The command link is deprecated.\u001b[0m\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, model symlinks are not supported anymore. You can\n",
            "load trained pipeline packages using their full names or from a directory\n",
            "path.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7ipay4bguxX",
        "outputId": "e546596f-5112-4f28-81c7-2c61384170ed"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhLYC0ivc0A_",
        "outputId": "eab561f2-4776-4c7d-9326-9431ec640009"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "# nlp = spacy.load('en_core_web_md')\n",
        "nlp = English() \n",
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7f82e8945910>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkXrmWA1ViH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f3b561-578f-4054-b038-36eea39a98cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPhP2TZdA_5l"
      },
      "source": [
        "# Answers Bleu and Rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjKjgdpqO8si"
      },
      "source": [
        "class report_generator():\n",
        "    def __init__(self, \n",
        "                 retriever_top_ks: list, \n",
        "                 reader_top_ks: list, \n",
        "                 embed_titles: list, \n",
        "                 reader_models: list,\n",
        "                 context_window_sizes: list,\n",
        "                 text_datasets: list,\n",
        "                 qa_datasets: list,\n",
        "                 max_seq_len_passages: list,\n",
        "                 max_seq_len_queries: list,\n",
        "                 doc_strides: list,\n",
        "                 max_seq_lens: list,\n",
        "                 report_out_dir: str,\n",
        "                 sample_out_dir: str):\n",
        "        \n",
        "        self.retriever_top_ks = retriever_top_ks\n",
        "        self.reader_top_ks = reader_top_ks\n",
        "        self.embed_titles = embed_titles\n",
        "        self.doc_strides = doc_strides\n",
        "        self.reader_models = reader_models\n",
        "        self.context_window_sizes = context_window_sizes\n",
        "        self.text_datasets = text_datasets\n",
        "        self.qa_datasets = qa_datasets\n",
        "        self.max_seq_len_passages = max_seq_len_passages\n",
        "        self.max_seq_len_queries = max_seq_len_queries\n",
        "        self.max_seq_lens = max_seq_lens\n",
        "        self.report_out_dir = report_out_dir\n",
        "        self.sample_out_dir = sample_out_dir\n",
        "        self.columns = ['QA model', 'Dataset', 'embed_title', 'context_window_size', \n",
        "                        'max_seq_len', 'doc_stride', 'max_seq_len_passage', 'max_seq_len_query',\n",
        "                        'Retriever top-k', 'QA top-k', 'Rouge-1', \n",
        "                        'Rouge-2', 'Rouge-l', 'Bleu', \n",
        "                        'Answers percent in retrieved documents', 'VRAM',\n",
        "                        'document embedding time', 'inference time'  \n",
        "                        ]\n",
        "\n",
        "    def __load_dataset(self, qa_dataset, text_dataset):\n",
        "        with open(qa_dataset, \"r\") as f:\n",
        "            self.qa = json.loads(f.read())['data']\n",
        "            df = pd.read_csv(text_dataset, index_col=0)\n",
        "            df = df.reset_index()\n",
        "\n",
        "        titles = list(df[\"title\"].values)\n",
        "        texts  = list(df[\"text\"].values)\n",
        "        self.documents: List[Document] = []\n",
        "        \n",
        "        for title, text in zip(titles, texts):\n",
        "            self.documents.append(\n",
        "                Document(\n",
        "                    text=text,\n",
        "                    meta={\n",
        "                        \"name\": title or \"\"\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "  \n",
        "    def __init_doc_store(self, similarity=\"dot_product\"):\n",
        "        self.document_store = FAISSDocumentStore(similarity=similarity,\n",
        "                                                faiss_index_factory_str=\"Flat\",\n",
        "                                                return_embedding=True\n",
        "                                                )\n",
        "\n",
        "    def __init_retriever(self, embed_title, \n",
        "                         query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "                         passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"):\n",
        "        \n",
        "        self.retriever = DensePassageRetriever(document_store=self.document_store,\n",
        "                                            query_embedding_model=query_embedding_model,\n",
        "                                            passage_embedding_model=passage_embedding_model,\n",
        "                                            use_gpu=True,\n",
        "                                            embed_title=embed_title\n",
        "                                            )\n",
        "    def __update_embedding(self):\n",
        "        self.document_store.delete_documents()\n",
        "        self.document_store.write_documents(self.documents)\n",
        "        self.document_store.update_embeddings(\n",
        "            retriever=self.retriever\n",
        "            )\n",
        "\n",
        "    def __init_reader(self, model_name, window_size, seq_len, stride):\n",
        "        self.reader = TransformersReader(model_name_or_path=model_name, \n",
        "                                        context_window_size=window_size,\n",
        "                                        max_seq_len=seq_len,\n",
        "                                        doc_stride=stride,\n",
        "                                        use_gpu=0\n",
        "                                        )\n",
        "        \n",
        "    def __compute_metrics(self, reader, retriever, qa, retriever_top_k, reader_top_k):\n",
        "        bleu_scores = []\n",
        "        rouge1_scores = []\n",
        "        rouge2_scores = []\n",
        "        rougel_scores = []\n",
        "        context_detection = []\n",
        "        context_accuracy = []\n",
        "\n",
        "        rouge = Rouge()\n",
        "        smoothie = SmoothingFunction().method4\n",
        "\n",
        "        for data in tqdm(qa):\n",
        "            true_context = data['paragraphs'][0]['context']\n",
        "            # true_context = true_context.replace('\\n', ' ')\n",
        "\n",
        "            for q_a in data['paragraphs'][0]['qas']:\n",
        "                question = q_a['question']\n",
        "                reference_list = set([answer['text'] for answer in q_a['answers']])\n",
        "                reference = \" \".join(reference_list)\n",
        "                reference_sents = nlp(reference)\n",
        "                reference_sents = list(reference_sents.sents)\n",
        "                reference_sents = [sent.text.lstrip().rstrip() for sent in reference_sents]\n",
        "\n",
        "                pipe = ExtractiveQAPipeline(reader, retriever)\n",
        "\n",
        "                preds = pipe.run(\n",
        "                    query=question,\n",
        "                    params={\"Retriever\": {\"top_k\": retriever_top_k}, \n",
        "                            \"Reader\": {\"top_k\": reader_top_k}}\n",
        "                    )\n",
        "\n",
        "                candidate_sent_list = []\n",
        "\n",
        "                for pred in preds['answers']:\n",
        "                    pred_answer = pred['answer']\n",
        "\n",
        "                    if pred_answer is not None:\n",
        "                        offset_start = pred['offset_start']\n",
        "                        offset_end = pred['offset_end']\n",
        "                        meta_name = pred['meta']['vector_id']\n",
        "\n",
        "                        pred_all_context_sents= []\n",
        "\n",
        "                        for pred in preds['documents']:\n",
        "                            pred_all_context_sents += list(nlp(pred.to_dict()['text']).sents)\n",
        "\n",
        "                            if pred.to_dict()['meta']['vector_id'] == meta_name:\n",
        "                                pred_context = pred.to_dict()['text']\n",
        "                                pred_context_sents = nlp(pred_context)\n",
        "                                pred_context_sents = list(pred_context_sents.sents)\n",
        "                                pred_context_sents = [sent.text for sent in pred_context_sents]\n",
        "                                # pred_context = \" \".join(pred_context_sents)\n",
        "\n",
        "                        pred_all_context_sents = [re.sub(r'\\n+', ' ', sent.text).strip() for sent in pred_all_context_sents]\n",
        "\n",
        "                        doc = nlp(pred_answer)\n",
        "                        pred_answer_sents = list(doc.sents)\n",
        "                        pred_answer_sents = [sent.text for sent in pred_answer_sents] \n",
        "\n",
        "                        for pred_context_sent in pred_context_sents:\n",
        "                            start_index = 0\n",
        "                            end_index = len(pred_answer) \n",
        "\n",
        "                            for pred_answer_sent in pred_answer_sents:\n",
        "                                right_reduction = len(pred_answer_sent) - len(pred_answer_sent.rstrip())\n",
        "                                left_reduction = len(pred_answer_sent) - len(pred_answer_sent.lstrip())\n",
        "                                end_index -= len(pred_answer_sent) + 0 if pred_context_sents[-1] == pred_answer_sent else 1\n",
        "\n",
        "                                context_offset_start = pred_context.find(pred_context_sent)\n",
        "                                context_offset_end = pred_context.find(pred_context_sent) + len(pred_context_sent)\n",
        "\n",
        "                                if  context_offset_start - left_reduction <= offset_start + start_index and context_offset_end + right_reduction >= offset_end - end_index:\n",
        "                                    candidate_sent_list.append(pred_context_sent)\n",
        "                                \n",
        "                                start_index += len(pred_answer_sent) + 0 if pred_context_sents[-1] == pred_answer_sent else 1\n",
        "\n",
        "                        for reference_sent in reference_sents:\n",
        "                            context_truth = 0\n",
        "\n",
        "                            if reference_sent in pred_all_context_sents:\n",
        "                                context_truth = 1\n",
        "                \n",
        "                            context_accuracy.append(context_truth)\n",
        "\n",
        "                candidate_sent_set = set(candidate_sent_list)\n",
        "                candidate = \" \".join(candidate_sent_set)\n",
        "                token_reference = nltk.word_tokenize(reference)\n",
        "                token_candidate = nltk.word_tokenize(candidate)\n",
        "\n",
        "                bleu_score = sentence_bleu(token_reference, \n",
        "                                            token_candidate, \n",
        "                                            smoothing_function=smoothie, \n",
        "                                            weights=(1, 0, 0, 0))\n",
        "                rouge_score = rouge.get_scores(candidate, reference)\n",
        "\n",
        "                bleu_scores.append(bleu_score)\n",
        "                rouge1_scores.append(rouge_score[0]['rouge-1']['f'])\n",
        "                rouge2_scores.append(rouge_score[0]['rouge-2']['f'])\n",
        "                rougel_scores.append(rouge_score[0]['rouge-l']['f'])\n",
        "\n",
        "        ctx_acc = context_accuracy.count(1)/len(context_accuracy)\n",
        "        bleu_ave = sum(bleu_scores)/len(bleu_scores)\n",
        "        rouge1_ave = sum(rouge1_scores)/len(rouge1_scores)\n",
        "        rouge2_ave = sum(rouge2_scores)/len(rouge2_scores)\n",
        "        rougel_ave = sum(rougel_scores)/len(rougel_scores)\n",
        "        # Pick the last question in dataset as a sample\n",
        "        return bleu_ave, rouge1_ave, rouge2_ave, rougel_ave, ctx_acc, question, reference_list, true_context, candidate_sent_set, pred_all_context_sents\n",
        "\n",
        "    def __serialize_sets(self, obj):\n",
        "        if isinstance(obj, set):\n",
        "            return list(obj)\n",
        "\n",
        "        return obj\n",
        "\n",
        "    def get_report(self):\n",
        "        logs = []\n",
        "        metric = [] \n",
        "\n",
        "        self.__init_doc_store()\n",
        "\n",
        "        for qa_dataset in self.qa_datasets:\n",
        "            for text_dataset in self.text_datasets:\n",
        "                self.__load_dataset(qa_dataset, text_dataset)\n",
        "\n",
        "                for embed_title in self.embed_titles:\n",
        "                    self.__init_retriever(embed_title)\n",
        "                    tic = time.time()\n",
        "                    self.__update_embedding()\n",
        "                    toc = time.time()\n",
        "                    document_embedding_time = toc - tic\n",
        "                    \n",
        "                    for reader_model in self.reader_models:\n",
        "                        for context_window_size in self.context_window_sizes:\n",
        "                            for doc_stride in self.doc_strides:\n",
        "                                for max_seq_len in self.max_seq_lens:\n",
        "                                    self.__init_reader(reader_model, context_window_size, max_seq_len, doc_stride)\n",
        "\n",
        "                                    for retriever_top_k in self.retriever_top_ks:\n",
        "                                        for reader_top_k in self.reader_top_ks:\n",
        "                                            for max_seq_len_passage in self.max_seq_len_passages:\n",
        "                                                for max_seq_len_query in self.max_seq_len_queries:\n",
        "                                                    tic = time.time()\n",
        "                                                    bleu_ave, rouge1_ave, rouge2_ave, rougel_ave, ctx_acc, question, reference_list, true_context, candidate_sent_set, pred_all_context_sents = self.__compute_metrics(self.reader, self.retriever, self.qa, retriever_top_k, reader_top_k)\n",
        "                                                    toc = time.time()\n",
        "                                                    inference_time = toc - tic\n",
        "\n",
        "                                                    cmd = \"nvidia-smi -q -x | grep \\<fb_memory_usage\\> -A 3 | grep used | sed -n 's:.*<used>\\(.*\\)</used>.*:\\1:p'\"\n",
        "                                                    vram = os.system(cmd)\n",
        "\n",
        "                                                    metric.append([reader_model, text_dataset, embed_title, context_window_size, max_seq_len, doc_stride,\n",
        "                                                                   max_seq_len_passage, max_seq_len_query, retriever_top_k, reader_top_k, rouge1_ave,\n",
        "                                                                   rouge2_ave, rougel_ave, bleu_ave, ctx_acc, vram, document_embedding_time, inference_time])\n",
        "                                                    \n",
        "                                                    log = {'Question': question,\n",
        "                                                           'Reference answers': reference_list,\n",
        "                                                           'Reference context': true_context,\n",
        "                                                           'Predicted answers': candidate_sent_set,\n",
        "                                                           'Retrieved context': pred_all_context_sents\n",
        "                                                           }\n",
        "\n",
        "                                                    logs.append(log)\n",
        "\n",
        "\n",
        "                    df = pd.DataFrame(metric, columns=self.columns)\n",
        "                    df.to_csv(self.report_out_dir, index=False)\n",
        "\n",
        "                    with open(self.sample_out_dir, 'w') as f:\n",
        "                        json.dump(logs, f, default=self.__serialize_sets)\n",
        "\n",
        "        return df, logs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrmjMbReuDBU"
      },
      "source": [
        "report_generator = report_generator(max_seq_lens = [256], \n",
        "                                    max_seq_len_passages = [256],   \n",
        "                                    max_seq_len_queries = [64],\n",
        "                                    embed_titles = [True, False], \n",
        "                                    context_window_sizes = [150, 175],\n",
        "                                    doc_strides = [100, 128],\n",
        "                                    retriever_top_ks = [3, 5, 7],\n",
        "                                    reader_top_ks = [3, 5, 7], \n",
        "                                    reader_models = ['drive/MyDrive/bert_basefi_qafi',\n",
        "                                                     'drive/MyDrive/squeeze-bertfi_qafi',\n",
        "                                                     'ktrapeznikov/albert-xlarge-v2-squad-v2',\n",
        "                                                     'deepset/roberta-base-squad2',\n",
        "                                                     'deepset/minilm-uncased-squad2',\n",
        "                                                     'ahotrod/albert_xxlargev1_squad2_512'],\n",
        "                                    text_datasets = ['drive/MyDrive/titleText-threeSentences.csv',\n",
        "                                                     'drive/MyDrive/titleText-paragraphs.csv'],\n",
        "                                    qa_datasets = ['drive/MyDrive/qa-SQUAD.json'],\n",
        "                                    report_out_dir = 'drive/MyDrive/dpr-qa-report.csv',\n",
        "                                    sample_out_dir = 'drive/MyDrive/dpr-qa-sample.json'\n",
        "                                    )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmspbmCyv0c5"
      },
      "source": [
        "df, logs = report_generator.get_report()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVszEdGfuC9i"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVw9FO8Xs9f7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}